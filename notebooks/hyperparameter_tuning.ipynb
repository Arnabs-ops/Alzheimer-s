{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab setup\n",
        "%pip install -q xgboost lightgbm shap optuna pyarrow category_encoders scikit-learn matplotlib seaborn joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Tuning for Alzheimer's Disease Prediction\n",
        "\n",
        "This notebook performs hyperparameter tuning using Optuna (with pruning and timeout) and also includes a quick RandomizedSearchCV path for fast experiments. It uses the shared utilities in `src/` and saves results to `results/`.\n",
        "\n",
        "Note: This notebook is set up to run end-to-end. You can choose either the full Optuna loop or the quick tuner.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import os, sys, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('./src')\n",
        "\n",
        "# Limit threads for stability\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['NUMEXPR_MAX_THREADS'] = '1'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "from src.hyper_tuning import run_optuna_tuning, run_random_search\n",
        "from src.advanced_model import load_real_data\n",
        "from src.validation import RobustValidator\n",
        "\n",
        "# Optional: install optuna if missing (uncomment to run)\n",
        "# %pip install optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and split data\n",
        "print('Loading data...')\n",
        "X, y = load_real_data()\n",
        "print(f'Shape X: {X.shape}, y: {y.shape}')\n",
        "\n",
        "validator = RobustValidator(random_state=42)\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = validator.create_fresh_splits(\n",
        "    X, y, test_size=0.2, val_size=0.2\n",
        ")\n",
        "\n",
        "# Clean data helper\n",
        "def clean_data(X):\n",
        "    X = np.array(X, dtype=np.float64)\n",
        "    X = np.where(np.isinf(X), np.nan, X)\n",
        "    X = np.where(np.abs(X) > 1e10, np.nan, X)\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X = imputer.fit_transform(X)\n",
        "    return X\n",
        "\n",
        "X_train = clean_data(X_train)\n",
        "X_val = clean_data(X_val)\n",
        "X_test = clean_data(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Optuna model builders\n",
        "\n",
        "def build_xgboost_model(trial):\n",
        "    import xgboost as xgb\n",
        "    return xgb.XGBClassifier(\n",
        "        n_estimators=trial.suggest_int('n_estimators', 50, 300, step=25),\n",
        "        max_depth=trial.suggest_int('max_depth', 3, 8),\n",
        "        learning_rate=trial.suggest_float('learning_rate', 0.03, 0.2, log=True),\n",
        "        subsample=trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        reg_alpha=trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
        "        reg_lambda=trial.suggest_float('reg_lambda', 0.1, 5.0),\n",
        "        gamma=trial.suggest_float('gamma', 0.0, 1.0),\n",
        "        min_child_weight=trial.suggest_int('min_child_weight', 1, 10),\n",
        "        eval_metric='logloss', use_label_encoder=False, random_state=42, verbosity=0, n_jobs=1\n",
        "    )\n",
        "\n",
        "\n",
        "def build_random_forest_model(trial):\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    return RandomForestClassifier(\n",
        "        n_estimators=trial.suggest_int('n_estimators', 50, 300, step=25),\n",
        "        max_depth=trial.suggest_int('max_depth', 4, 15),\n",
        "        min_samples_split=trial.suggest_int('min_samples_split', 2, 20),\n",
        "        min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "        max_features=trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.3, 0.5, 0.7]),\n",
        "        bootstrap=trial.suggest_categorical('bootstrap', [True, False]),\n",
        "        random_state=42, n_jobs=1\n",
        "    )\n",
        "\n",
        "\n",
        "def build_lightgbm_model(trial):\n",
        "    import lightgbm as lgb\n",
        "    return lgb.LGBMClassifier(\n",
        "        n_estimators=trial.suggest_int('n_estimators', 50, 300, step=25),\n",
        "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
        "        learning_rate=trial.suggest_float('learning_rate', 0.03, 0.2, log=True),\n",
        "        subsample=trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        reg_alpha=trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
        "        reg_lambda=trial.suggest_float('reg_lambda', 0.1, 5.0),\n",
        "        num_leaves=trial.suggest_int('num_leaves', 10, 100),\n",
        "        min_child_samples=trial.suggest_int('min_child_samples', 5, 50),\n",
        "        random_state=42, verbose=-1, n_jobs=1\n",
        "    )\n",
        "\n",
        "\n",
        "def build_svm_model(trial):\n",
        "    from sklearn.svm import SVC\n",
        "    kernel = trial.suggest_categorical('kernel', ['rbf', 'poly', 'sigmoid'])\n",
        "    degree = trial.suggest_int('degree', 2, 5) if kernel == 'poly' else 3\n",
        "    return SVC(\n",
        "        C=trial.suggest_float('C', 0.01, 100, log=True),\n",
        "        gamma=trial.suggest_categorical('gamma', ['scale', 'auto', 0.001, 0.01, 0.1, 1.0]),\n",
        "        kernel=kernel,\n",
        "        degree=degree,\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "\n",
        "def build_logistic_regression_model(trial):\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet'])\n",
        "    solver = 'saga' if penalty in ['l1', 'elasticnet'] else 'liblinear'\n",
        "    return LogisticRegression(\n",
        "        C=trial.suggest_float('C', 0.001, 10, log=True),\n",
        "        penalty=penalty,\n",
        "        solver=solver,\n",
        "        l1_ratio=trial.suggest_float('l1_ratio', 0.1, 0.9) if penalty == 'elasticnet' else None,\n",
        "        max_iter=500,\n",
        "        random_state=42,\n",
        "        n_jobs=1\n",
        "    )\n",
        "\n",
        "\n",
        "def build_mlp_model(trial):\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    hidden_layers = trial.suggest_int('n_layers', 1, 3)\n",
        "    hidden_sizes = [trial.suggest_int(f'layer_{i}_size', 20, 200) for i in range(hidden_layers)]\n",
        "    return MLPClassifier(\n",
        "        hidden_layer_sizes=tuple(hidden_sizes),\n",
        "        activation=trial.suggest_categorical('activation', ['relu', 'tanh', 'logistic']),\n",
        "        solver=trial.suggest_categorical('solver', ['adam', 'lbfgs']),\n",
        "        alpha=trial.suggest_float('alpha', 1e-5, 1e-1, log=True),\n",
        "        learning_rate=trial.suggest_categorical('learning_rate', ['constant', 'adaptive']),\n",
        "        max_iter=500,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "models_to_tune = {\n",
        "    'XGBoost': build_xgboost_model,\n",
        "    'Random Forest': build_random_forest_model,\n",
        "    'LightGBM': build_lightgbm_model,\n",
        "    'SVM': build_svm_model,\n",
        "    'Logistic Regression': build_logistic_regression_model,\n",
        "    'MLP': build_mlp_model\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optuna Tuning (Full)\n",
        "- 30 trials per model, 10 minute timeout per model\n",
        "- Pruning enabled to skip weak trials\n",
        "- Uses 3-fold Stratified CV\n",
        "- Saves best params and validation/test scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optuna Tuning (Full)\n",
        "print(\"🔧 Starting Optuna hyperparameter tuning...\")\n",
        "\n",
        "optuna_results = {}\n",
        "\n",
        "for model_name, model_builder in models_to_tune.items():\n",
        "    print(f\"\\n🔧 Tuning {model_name}...\")\n",
        "    \n",
        "    try:\n",
        "        best_model, tuning_info = run_optuna_tuning(\n",
        "            model_builder=model_builder,\n",
        "            model_name=model_name,\n",
        "            X=X_train,\n",
        "            y=y_train,\n",
        "            n_trials=30,\n",
        "            timeout=600,  # 10 minutes\n",
        "            cv_folds=3,\n",
        "            n_jobs=1,\n",
        "            random_state=42,\n",
        "            enable_pruning=True,\n",
        "            verbose=True\n",
        "        )\n",
        "        \n",
        "        if best_model is not None:\n",
        "            # Evaluate on validation set\n",
        "            val_score = best_model.score(X_val, y_val)\n",
        "            test_score = best_model.score(X_test, y_test)\n",
        "            \n",
        "            optuna_results[model_name] = {\n",
        "                'best_model': best_model,\n",
        "                'val_score': val_score,\n",
        "                'test_score': test_score,\n",
        "                'tuning_info': tuning_info\n",
        "            }\n",
        "            \n",
        "            print(f\"✅ {model_name} tuned successfully!\")\n",
        "            print(f\"  Validation Score: {val_score:.4f}\")\n",
        "            print(f\"  Test Score: {test_score:.4f}\")\n",
        "            print(f\"  Best Score: {tuning_info['best_score']:.4f}\")\n",
        "            print(f\"  Trials: {tuning_info['n_trials_completed']}\")\n",
        "            print(f\"  Runtime: {tuning_info['runtime_seconds']:.2f}s\")\n",
        "        else:\n",
        "            print(f\"⚠️ {model_name} tuning failed - no valid trials\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"❌ {model_name} tuning failed: {e}\")\n",
        "\n",
        "print(f\"\\n✅ Optuna tuning complete! {len(optuna_results)} models tuned successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick Tuning (RandomizedSearchCV)\n",
        "print(\"⚡ Starting quick RandomizedSearchCV tuning...\")\n",
        "\n",
        "quick_results = {}\n",
        "\n",
        "# Select a subset of models for quick tuning\n",
        "quick_models = ['XGBoost', 'Random Forest', 'LightGBM']\n",
        "\n",
        "for model_name in quick_models:\n",
        "    if model_name in models_to_tune:\n",
        "        print(f\"\\n⚡ Quick tuning {model_name}...\")\n",
        "        \n",
        "        try:\n",
        "            # Get base model\n",
        "            base_model = models_to_tune[model_name](None)  # Dummy trial for base model\n",
        "            \n",
        "            best_model, tuning_info = run_random_search(\n",
        "                model=base_model,\n",
        "                model_name=model_name,\n",
        "                X=X_train,\n",
        "                y=y_train,\n",
        "                n_iter=15,  # Fewer iterations for speed\n",
        "                cv_folds=3,\n",
        "                n_jobs=1,\n",
        "                random_state=42\n",
        "            )\n",
        "            \n",
        "            if best_model is not None:\n",
        "                # Evaluate on validation set\n",
        "                val_score = best_model.score(X_val, y_val)\n",
        "                test_score = best_model.score(X_test, y_test)\n",
        "                \n",
        "                quick_results[model_name] = {\n",
        "                    'best_model': best_model,\n",
        "                    'val_score': val_score,\n",
        "                    'test_score': test_score,\n",
        "                    'tuning_info': tuning_info\n",
        "                }\n",
        "                \n",
        "                print(f\"✅ {model_name} quick tuned!\")\n",
        "                print(f\"  Validation Score: {val_score:.4f}\")\n",
        "                print(f\"  Test Score: {test_score:.4f}\")\n",
        "                print(f\"  Best Score: {tuning_info['best_score']:.4f}\")\n",
        "            else:\n",
        "                print(f\"⚠️ {model_name} quick tuning failed\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"❌ {model_name} quick tuning failed: {e}\")\n",
        "\n",
        "print(f\"\\n✅ Quick tuning complete! {len(quick_results)} models tuned.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Results\n",
        "print(\"📊 Comparing tuning results...\")\n",
        "\n",
        "# Combine results\n",
        "all_results = {}\n",
        "\n",
        "# Add Optuna results\n",
        "for model_name, results in optuna_results.items():\n",
        "    all_results[f\"{model_name} (Optuna)\"] = results\n",
        "\n",
        "# Add Quick results\n",
        "for model_name, results in quick_results.items():\n",
        "    all_results[f\"{model_name} (Quick)\"] = results\n",
        "\n",
        "if all_results:\n",
        "    # Create comparison table\n",
        "    comparison_data = []\n",
        "    for model_name, results in all_results.items():\n",
        "        comparison_data.append({\n",
        "            'Model': model_name,\n",
        "            'Val_Score': results['val_score'],\n",
        "            'Test_Score': results['test_score'],\n",
        "            'Best_CV_Score': results['tuning_info']['best_score'],\n",
        "            'Tuning_Time': results['tuning_info'].get('runtime_seconds', 0)\n",
        "        })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    comparison_df = comparison_df.sort_values('Test_Score', ascending=False)\n",
        "    \n",
        "    print(\"\\n📊 Tuning Results Comparison:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Find best model\n",
        "    best_model_name = comparison_df.iloc[0]['Model']\n",
        "    best_model = all_results[best_model_name]['best_model']\n",
        "    best_score = comparison_df.iloc[0]['Test_Score']\n",
        "    \n",
        "    print(f\"\\n🏆 Best tuned model: {best_model_name}\")\n",
        "    print(f\"📊 Test Score: {best_score:.4f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ No tuning results available for comparison\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Results\n",
        "print(\"💾 Saving hyperparameter tuning results...\")\n",
        "\n",
        "# Save comparison results\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "if all_results:\n",
        "    # Save comparison CSV\n",
        "    csv_path = f'results/hyperparameter_tuning_summary_{timestamp}.csv'\n",
        "    comparison_df.to_csv(csv_path, index=False)\n",
        "    print(f\"✅ Comparison results saved to: {csv_path}\")\n",
        "    \n",
        "    # Save best tuned model\n",
        "    best_model_path = f'results/best_tuned_model_{timestamp}.pkl'\n",
        "    import joblib\n",
        "    joblib.dump(best_model, best_model_path)\n",
        "    print(f\"✅ Best tuned model saved to: {best_model_path}\")\n",
        "    \n",
        "    # Save detailed tuning info\n",
        "    detailed_info = {}\n",
        "    for model_name, results in all_results.items():\n",
        "        detailed_info[model_name] = {\n",
        "            'val_score': float(results['val_score']),\n",
        "            'test_score': float(results['test_score']),\n",
        "            'best_cv_score': float(results['tuning_info']['best_score']),\n",
        "            'tuning_time': float(results['tuning_info'].get('runtime_seconds', 0)),\n",
        "            'best_params': results['tuning_info'].get('best_params', {})\n",
        "        }\n",
        "    \n",
        "    json_path = f'results/hyperparameter_tuning_details_{timestamp}.json'\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(detailed_info, f, indent=2)\n",
        "    print(f\"✅ Detailed tuning info saved to: {json_path}\")\n",
        "    \n",
        "    print(f\"\\n🎉 Hyperparameter tuning complete!\")\n",
        "    print(f\"🏆 Best model: {best_model_name} (Test Score: {best_score:.4f})\")\n",
        "    print(f\"📊 All results saved to results/ directory\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️ No results to save\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optuna Tuning (Full)\n",
        "- 30 trials per model, 10 minute timeout per model\n",
        "- Pruning enabled to skip weak trials\n",
        "- Uses 3-fold Stratified CV\n",
        "- Saves best params and validation/test scores\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
