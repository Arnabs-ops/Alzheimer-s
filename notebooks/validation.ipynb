{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab setup\n",
        "%pip install -q xgboost lightgbm shap pyarrow scikit-learn matplotlib seaborn joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Robust Validation for Alzheimer's Disease Models\n",
        "\n",
        "This notebook performs comprehensive validation using stratified K-fold cross-validation, learning curves, and bootstrap confidence intervals to ensure model reliability and generalization.\n",
        "\n",
        "## Features:\n",
        "- Fresh train/validation/test splits\n",
        "- Stratified K-fold cross-validation\n",
        "- Learning curves analysis\n",
        "- Bootstrap confidence intervals (20 iterations, interrupt-safe)\n",
        "- Overfitting detection\n",
        "- Validation summary reports\n",
        "\n",
        "## Outputs:\n",
        "- Validation plots and learning curves\n",
        "- `validation_summary.json` with comprehensive metrics\n",
        "- Bootstrap confidence intervals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('./src')\n",
        "\n",
        "# Set thread limits for stability\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['NUMEXPR_MAX_THREADS'] = '1'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Import our validation module\n",
        "from src.validation import RobustValidator\n",
        "from src.advanced_model import AdvancedAlzheimerModel\n",
        "\n",
        "# Create results directory\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Setup complete - Ready for robust validation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data\n",
        "print(\"üìä Loading data for validation...\")\n",
        "\n",
        "# Initialize validator\n",
        "validator = RobustValidator(random_state=42)\n",
        "\n",
        "# Load data\n",
        "try:\n",
        "    # Try NPZ first\n",
        "    data = np.load('data/processed/preprocessed_alz_data.npz', allow_pickle=True)\n",
        "    X = np.vstack([data['X_train'], data['X_test']])\n",
        "    y = np.concatenate([data['y_train'], data['y_test']])\n",
        "    \n",
        "    # Handle multi-dimensional y\n",
        "    if len(y.shape) > 1:\n",
        "        if y.shape[1] == 1:\n",
        "            y = y.ravel()\n",
        "        else:\n",
        "            y = np.argmax(y, axis=1)\n",
        "    \n",
        "    print(f\"‚úÖ Loaded NPZ data: {X.shape}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è NPZ loading failed: {e}\")\n",
        "    print(\"üîÑ Creating sample data...\")\n",
        "    \n",
        "    # Create sample data\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(1200, 50)\n",
        "    y = np.random.choice([0, 1, 2], 1200)\n",
        "    print(f\"‚úÖ Created sample data: {X.shape}\")\n",
        "\n",
        "print(f\"üìä Data shape: {X.shape}\")\n",
        "print(f\"üìä Target distribution: {np.bincount(y)}\")\n",
        "print(f\"üìä Classes: {len(np.unique(y))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Fresh Splits\n",
        "print(\"üîÑ Creating fresh train/validation/test splits...\")\n",
        "\n",
        "# Clean data helper\n",
        "def clean_data(X):\n",
        "    X = np.array(X, dtype=np.float64)\n",
        "    X = np.where(np.isinf(X), np.nan, X)\n",
        "    X = np.where(np.abs(X) > 1e10, np.nan, X)\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X = imputer.fit_transform(X)\n",
        "    return X\n",
        "\n",
        "# Clean the data\n",
        "X = clean_data(X)\n",
        "\n",
        "# Create fresh splits\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = validator.create_fresh_splits(\n",
        "    X, y, test_size=0.2, val_size=0.2\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Fresh splits created:\")\n",
        "print(f\"  Train: {X_train.shape} (labels: {y_train.shape})\")\n",
        "print(f\"  Validation: {X_val.shape} (labels: {y_val.shape})\")\n",
        "print(f\"  Test: {X_test.shape} (labels: {y_test.shape})\")\n",
        "\n",
        "# Check class distribution\n",
        "print(f\"\\nüìä Class distribution:\")\n",
        "print(f\"  Train: {np.bincount(y_train)}\")\n",
        "print(f\"  Validation: {np.bincount(y_val)}\")\n",
        "print(f\"  Test: {np.bincount(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Validation Report\n",
        "print(\"üìä Running comprehensive validation...\")\n",
        "\n",
        "# Initialize advanced model\n",
        "advanced_model = AdvancedAlzheimerModel(random_state=42)\n",
        "\n",
        "# Run comprehensive validation\n",
        "validation_results = validator.comprehensive_validation_report(\n",
        "    advanced_model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "    cv_folds=3, n_bootstrap=20\n",
        ")\n",
        "\n",
        "print(\"\\nüìä Validation Results Summary:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Display results\n",
        "for model_name, results in validation_results.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Train Score: {results['train_score']:.4f}\")\n",
        "    print(f\"  Val Score: {results['val_score']:.4f}\")\n",
        "    print(f\"  Test Score: {results['test_score']:.4f}\")\n",
        "    print(f\"  CV Mean: {results['cv_mean']:.4f} ¬± {results['cv_std']:.4f}\")\n",
        "    print(f\"  Gap (Train-Val): {results['train_val_gap']:.4f}\")\n",
        "    print(f\"  Gap (Train-Test): {results['train_test_gap']:.4f}\")\n",
        "    \n",
        "    if 'bootstrap_ci' in results:\n",
        "        ci = results['bootstrap_ci']\n",
        "        print(f\"  Bootstrap CI (95%): [{ci['ci_95_lower']:.4f}, {ci['ci_95_upper']:.4f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Learning Curves Analysis\n",
        "print(\"üìà Generating learning curves...\")\n",
        "\n",
        "# Plot learning curves for key models\n",
        "key_models = ['Random Forest (Regularized)', 'XGBoost (Regularized)', 'Logistic Regression (L1)']\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, model_name in enumerate(key_models):\n",
        "    if model_name in validation_results and 'learning_curve' in validation_results[model_name]:\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        \n",
        "        lc_data = validation_results[model_name]['learning_curve']\n",
        "        train_sizes = lc_data['train_sizes']\n",
        "        train_scores = lc_data['train_scores']\n",
        "        val_scores = lc_data['val_scores']\n",
        "        \n",
        "        plt.plot(train_sizes, train_scores, 'o-', label='Training Score', alpha=0.7)\n",
        "        plt.plot(train_sizes, val_scores, 'o-', label='Validation Score', alpha=0.7)\n",
        "        plt.fill_between(train_sizes, \n",
        "                        np.array(train_scores) - np.array(lc_data['train_scores_std']),\n",
        "                        np.array(train_scores) + np.array(lc_data['train_scores_std']),\n",
        "                        alpha=0.1)\n",
        "        plt.fill_between(train_sizes,\n",
        "                        np.array(val_scores) - np.array(lc_data['val_scores_std']),\n",
        "                        np.array(val_scores) + np.array(lc_data['val_scores_std']),\n",
        "                        alpha=0.1)\n",
        "        \n",
        "        plt.title(f'Learning Curve - {model_name}')\n",
        "        plt.xlabel('Training Set Size')\n",
        "        plt.ylabel('Score')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Learning curves generated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overfitting Analysis\n",
        "print(\"üîç Analyzing overfitting patterns...\")\n",
        "\n",
        "# Create overfitting analysis plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Extract data for plotting\n",
        "model_names = list(validation_results.keys())\n",
        "train_scores = [validation_results[name]['train_score'] for name in model_names]\n",
        "val_scores = [validation_results[name]['val_score'] for name in model_names]\n",
        "test_scores = [validation_results[name]['test_score'] for name in model_names]\n",
        "gaps = [validation_results[name]['train_val_gap'] for name in model_names]\n",
        "\n",
        "# Plot 1: Score comparison\n",
        "plt.subplot(2, 2, 1)\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.25\n",
        "plt.bar(x - width, train_scores, width, label='Train', alpha=0.7)\n",
        "plt.bar(x, val_scores, width, label='Validation', alpha=0.7)\n",
        "plt.bar(x + width, test_scores, width, label='Test', alpha=0.7)\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks(x, [name.replace(' (Regularized)', '') for name in model_names], rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Overfitting gaps\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.bar(range(len(model_names)), gaps, alpha=0.7, color='red')\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Train-Val Gap')\n",
        "plt.title('Overfitting Analysis (Train-Val Gap)')\n",
        "plt.xticks(range(len(model_names)), [name.replace(' (Regularized)', '') for name in model_names], rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: CV scores with error bars\n",
        "plt.subplot(2, 2, 3)\n",
        "cv_means = [validation_results[name]['cv_mean'] for name in model_names]\n",
        "cv_stds = [validation_results[name]['cv_std'] for name in model_names]\n",
        "plt.errorbar(range(len(model_names)), cv_means, yerr=cv_stds, fmt='o', capsize=5)\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('CV Score')\n",
        "plt.title('Cross-Validation Scores')\n",
        "plt.xticks(range(len(model_names)), [name.replace(' (Regularized)', '') for name in model_names], rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Bootstrap confidence intervals\n",
        "plt.subplot(2, 2, 4)\n",
        "for i, name in enumerate(model_names):\n",
        "    if 'bootstrap_ci' in validation_results[name]:\n",
        "        ci = validation_results[name]['bootstrap_ci']\n",
        "        plt.errorbar(i, ci['mean'], \n",
        "                    yerr=[[ci['mean'] - ci['ci_95_lower']], [ci['ci_95_upper'] - ci['mean']]], \n",
        "                    fmt='o', capsize=5, label=name.replace(' (Regularized)', ''))\n",
        "\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Bootstrap Score')\n",
        "plt.title('Bootstrap Confidence Intervals')\n",
        "plt.xticks(range(len(model_names)), [name.replace(' (Regularized)', '') for name in model_names], rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Overfitting analysis complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Validation Results\n",
        "print(\"üíæ Saving validation results...\")\n",
        "\n",
        "# Save comprehensive validation summary\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "json_path = f'results/validation_summary_{timestamp}.json'\n",
        "\n",
        "# Convert numpy types to Python types for JSON serialization\n",
        "def convert_numpy_types(obj):\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy_types(item) for item in obj]\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Clean the results for JSON serialization\n",
        "clean_results = convert_numpy_types(validation_results)\n",
        "\n",
        "with open(json_path, 'w') as f:\n",
        "    json.dump(clean_results, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Validation summary saved to: {json_path}\")\n",
        "\n",
        "# Create summary table\n",
        "summary_data = []\n",
        "for model_name, results in validation_results.items():\n",
        "    summary_data.append({\n",
        "        'Model': model_name.replace(' (Regularized)', ''),\n",
        "        'Train_Score': results['train_score'],\n",
        "        'Val_Score': results['val_score'],\n",
        "        'Test_Score': results['test_score'],\n",
        "        'CV_Mean': results['cv_mean'],\n",
        "        'CV_Std': results['cv_std'],\n",
        "        'Train_Val_Gap': results['train_val_gap'],\n",
        "        'Train_Test_Gap': results['train_test_gap']\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df = summary_df.sort_values('Test_Score', ascending=False)\n",
        "\n",
        "# Save summary CSV\n",
        "csv_path = f'results/validation_summary_{timestamp}.csv'\n",
        "summary_df.to_csv(csv_path, index=False)\n",
        "print(f\"‚úÖ Validation summary CSV saved to: {csv_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Validation complete!\")\n",
        "print(f\"üìä Best performing model: {summary_df.iloc[0]['Model']} (Test Score: {summary_df.iloc[0]['Test_Score']:.4f})\")\n",
        "print(f\"üìä All validation results saved to results/ directory\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
