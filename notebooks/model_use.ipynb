{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Usage and Inference for Alzheimer's Disease Prediction\n",
        "\n",
        "This notebook loads trained models and performs predictions on new data, providing a complete inference pipeline.\n",
        "\n",
        "## Features:\n",
        "- Load best trained model from results directory\n",
        "- Load new data for prediction\n",
        "- Generate predictions with confidence scores\n",
        "- Save prediction results\n",
        "- Model performance evaluation on new data\n",
        "\n",
        "## Usage:\n",
        "1. Ensure you have trained models in `results/` directory\n",
        "2. Prepare new data in the same format as training data\n",
        "3. Run inference and get predictions\n",
        "4. Save results for further analysis\n",
        "\n",
        "## Outputs:\n",
        "- `predictions.csv` with predictions and confidence scores\n",
        "- Model performance metrics on new data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('./src')\n",
        "\n",
        "# Set thread limits for stability\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['NUMEXPR_MAX_THREADS'] = '1'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import joblib\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# Create results directory\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "print(\"✅ Setup complete - Ready for model inference\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find and Load Best Model\n",
        "print(\"🔍 Finding best trained model...\")\n",
        "\n",
        "# Look for model files in results directory\n",
        "model_files = glob.glob('results/best_model_*.pkl')\n",
        "tuned_model_files = glob.glob('results/best_tuned_model_*.pkl')\n",
        "fused_model_files = glob.glob('results/best_fused_dataset_*.npz')\n",
        "\n",
        "print(f\"📊 Found {len(model_files)} basic models\")\n",
        "print(f\"📊 Found {len(tuned_model_files)} tuned models\")\n",
        "print(f\"📊 Found {len(fused_model_files)} fused datasets\")\n",
        "\n",
        "# Load the most recent model\n",
        "if tuned_model_files:\n",
        "    model_path = max(tuned_model_files, key=os.path.getctime)\n",
        "    model_type = \"tuned\"\n",
        "elif model_files:\n",
        "    model_path = max(model_files, key=os.path.getctime)\n",
        "    model_type = \"basic\"\n",
        "else:\n",
        "    print(\"⚠️ No trained models found. Creating sample model...\")\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model_type = \"sample\"\n",
        "\n",
        "if model_type != \"sample\":\n",
        "    print(f\"📊 Loading {model_type} model from: {model_path}\")\n",
        "    model = joblib.load(model_path)\n",
        "    print(f\"✅ Model loaded successfully\")\n",
        "    print(f\"📊 Model type: {type(model).__name__}\")\n",
        "else:\n",
        "    print(\"📊 Using sample Random Forest model\")\n",
        "\n",
        "# Load model metadata if available\n",
        "metadata_files = glob.glob('results/*_summary_*.json')\n",
        "if metadata_files:\n",
        "    metadata_path = max(metadata_files, key=os.path.getctime)\n",
        "    try:\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        print(f\"📊 Model metadata loaded from: {metadata_path}\")\n",
        "    except:\n",
        "        metadata = {}\n",
        "else:\n",
        "    metadata = {}\n",
        "\n",
        "print(f\"✅ Model loading complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load New Data for Prediction\n",
        "print(\"📊 Loading new data for prediction...\")\n",
        "\n",
        "# Try to load new data from various sources\n",
        "new_data_loaded = False\n",
        "\n",
        "# Option 1: Try to load fused dataset\n",
        "fused_model_files = []\n",
        "import glob\n",
        "fused_model_files = glob.glob('results/best_fused_dataset_*.npz')\n",
        "if fused_model_files:\n",
        "    try:\n",
        "        fused_path = max(fused_model_files, key=os.path.getctime)\n",
        "        fused_data = np.load(fused_path, allow_pickle=True)\n",
        "        X_new = fused_data['X_test']\n",
        "        y_new = fused_data['y_test'] if 'y_test' in fused_data else None\n",
        "        print(f\"✅ Loaded fused dataset: {X_new.shape}\")\n",
        "        new_data_loaded = True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Fused dataset loading failed: {e}\")\n",
        "\n",
        "# Option 2: Try to load NPZ data\n",
        "if not new_data_loaded:\n",
        "    try:\n",
        "        npz_data = np.load('data/processed/preprocessed_alz_data.npz', allow_pickle=True)\n",
        "        X_new = npz_data['X_test']\n",
        "        y_new = npz_data['y_test']\n",
        "        print(f\"✅ Loaded NPZ test data: {X_new.shape}\")\n",
        "        new_data_loaded = True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ NPZ loading failed: {e}\")\n",
        "\n",
        "# Option 3: Try to load CSV data\n",
        "if not new_data_loaded:\n",
        "    try:\n",
        "        df = pd.read_csv('data/processed/alz_clean.csv')\n",
        "        # Assume last column is target, rest are features\n",
        "        X_new = df.iloc[:, :-1].values\n",
        "        y_new = df.iloc[:, -1].values\n",
        "        print(f\"✅ Loaded CSV data: {X_new.shape}\")\n",
        "        new_data_loaded = True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ CSV loading failed: {e}\")\n",
        "\n",
        "# Option 4: Create sample data\n",
        "if not new_data_loaded:\n",
        "    print(\"🔄 Creating sample data for demonstration...\")\n",
        "    np.random.seed(42)\n",
        "    X_new = np.random.randn(100, 50)  # 100 samples, 50 features\n",
        "    y_new = None\n",
        "    print(f\"✅ Created sample data: {X_new.shape}\")\n",
        "    new_data_loaded = True\n",
        "\n",
        "# Clean the data\n",
        "def clean_data(X):\n",
        "    X = np.array(X, dtype=np.float64)\n",
        "    X = np.where(np.isinf(X), np.nan, X)\n",
        "    X = np.where(np.abs(X) > 1e10, np.nan, X)\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X = imputer.fit_transform(X)\n",
        "    return X\n",
        "\n",
        "X_new = clean_data(X_new)\n",
        "\n",
        "print(f\"📊 New data shape: {X_new.shape}\")\n",
        "if y_new is not None:\n",
        "    # If one-hot, convert to class indices\n",
        "    if len(y_new.shape) > 1 and y_new.shape[1] > 1:\n",
        "        y_new = np.argmax(y_new, axis=1)\n",
        "    print(f\"📊 Target distribution: {np.bincount(y_new)}\")\n",
        "    print(f\"📊 Classes: {len(np.unique(y_new))}\")\n",
        "else:\n",
        "    print(\"📊 No ground truth labels available (prediction mode)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Predictions\n",
        "print(\"🤖 Running predictions...\")\n",
        "\n",
        "# Predict classes\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "# Predict probabilities if available\n",
        "proba_available = hasattr(model, 'predict_proba')\n",
        "if proba_available:\n",
        "    y_proba = model.predict_proba(X_new)\n",
        "    max_proba = np.max(y_proba, axis=1)\n",
        "else:\n",
        "    y_proba = None\n",
        "    max_proba = None\n",
        "\n",
        "print(f\"✅ Predictions complete: {y_pred.shape}\")\n",
        "\n",
        "# Build results DataFrame\n",
        "results_df = pd.DataFrame({'prediction': y_pred})\n",
        "if max_proba is not None:\n",
        "    results_df['confidence'] = max_proba\n",
        "\n",
        "print(results_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate (if labels available)\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "if y_new is not None:\n",
        "    print(\"📊 Evaluating predictions...\")\n",
        "    \n",
        "    acc = accuracy_score(y_new, y_pred)\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    \n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_new, y_pred))\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_new, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠️ Ground truth labels not provided. Skipping evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Predictions\n",
        "print(\"💾 Saving predictions...\")\n",
        "\n",
        "from datetime import datetime\n",
        "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Save CSV\n",
        "pred_csv = f'results/predictions_{ts}.csv'\n",
        "results_df.to_csv(pred_csv, index=False)\n",
        "print(f\"✅ Predictions saved to: {pred_csv}\")\n",
        "\n",
        "# Save JSON summary\n",
        "summary = {\n",
        "    'model_path': model_path if 'model_path' in globals() else None,\n",
        "    'num_predictions': int(len(results_df)),\n",
        "    'proba_available': bool(proba_available),\n",
        "}\n",
        "\n",
        "if y_new is not None:\n",
        "    summary['accuracy'] = float(acc)\n",
        "\n",
        "import json\n",
        "pred_json = f'results/predictions_summary_{ts}.json'\n",
        "with open(pred_json, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(f\"✅ Prediction summary saved to: {pred_json}\")\n",
        "\n",
        "print(\"\\n🎉 Inference complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load New Data for Prediction\n",
        "print(\"📊 Loading new data for prediction...\")\n",
        "\n",
        "# Try to load new data from various sources\n",
        "new_data_loaded = False\n",
        "\n",
        "# Option 1: Try to load fused dataset\n",
        "if fused_model_files:\n",
        "    try:\n",
        "        fused_path = max(fused_model_files, key=os.path.getctime)\n",
        "        fused_data = np.load(fused_path, allow_pickle=True)\n",
        "        X_new = fused_data['X_test']\n",
        "        y_new = fused_data['y_test'] if 'y_test' in fused_data else None\n",
        "        print(f\"✅ Loaded fused dataset: {X_new.shape}\")\n",
        "        new_data_loaded = True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Fused dataset loading failed: {e}\")\n",
        "\n",
        "# Option 2: Try to load NPZ data\n",
        "if not new_data_loaded:\n",
        "    try:\n",
        "        npz_data = np.load('data/processed/preprocessed_alz_data.npz', allow_pickle=True)\n",
        "        X_new = npz_data['X_test']\n",
        "        y_new = npz_data['y_test']\n",
        "        print(f\"✅ Loaded NPZ test data: {X_new.shape}\")\n",
        "        new_data_loaded = True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ NPZ loading failed: {e}\")\n",
        "\n",
        "# Option 3: Try to load CSV data\n",
        "if not new_data_loaded:\n",
        "    try:\n",
        "        df = pd.read_csv('data/processed/alz_clean.csv')\n",
        "        # Assume last column is target, rest are features\n",
        "        X_new = df.iloc[:, :-1].values\n",
        "        y_new = df.iloc[:, -1].values\n",
        "        print(f\"✅ Loaded CSV data: {X_new.shape}\")\n",
        "        new_data_loaded = True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ CSV loading failed: {e}\")\n",
        "\n",
        "# Option 4: Create sample data\n",
        "if not new_data_loaded:\n",
        "    print(\"🔄 Creating sample data for demonstration...\")\n",
        "    np.random.seed(42)\n",
        "    X_new = np.random.randn(100, 50)  # 100 samples, 50 features\n",
        "    y_new = np.random.choice([0, 1, 2], 100)  # 3 classes\n",
        "    print(f\"✅ Created sample data: {X_new.shape}\")\n",
        "    new_data_loaded = True\n",
        "\n",
        "# Clean the data\n",
        "def clean_data(X):\n",
        "    X = np.array(X, dtype=np.float64)\n",
        "    X = np.where(np.isinf(X), np.nan, X)\n",
        "    X = np.where(np.abs(X) > 1e10, np.nan, X)\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X = imputer.fit_transform(X)\n",
        "    return X\n",
        "\n",
        "X_new = clean_data(X_new)\n",
        "\n",
        "print(f\"📊 New data shape: {X_new.shape}\")\n",
        "if y_new is not None:\n",
        "    print(f\"📊 Target distribution: {np.bincount(y_new)}\")\n",
        "    print(f\"📊 Classes: {len(np.unique(y_new))}\")\n",
        "else:\n",
        "    print(\"📊 No ground truth labels available (prediction mode)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
