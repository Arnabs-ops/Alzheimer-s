{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Clone repository (Colab only - skip if running locally)\n",
        "try:\n",
        "    import google.colab\n",
        "    !git clone https://github.com/Arnabs-ops/Alzheimer-s.git\n",
        "    %cd Alzheimer-s\n",
        "    print(\"‚úÖ Repository cloned and directory changed\")\n",
        "except:\n",
        "    print(\"‚ÑπÔ∏è Running locally - skipping git clone\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Install dependencies (Colab setup)\n",
        "%pip install -q xgboost lightgbm shap pyarrow category_encoders scikit-learn matplotlib seaborn joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Core AI Training for Alzheimer's Disease Prediction\n",
        "\n",
        "This notebook trains baseline machine learning models on preprocessed genomic data and provides comprehensive evaluation with interpretability analysis.\n",
        "\n",
        "## Features:\n",
        "- Load preprocessed NPZ data or fallback to CSV\n",
        "- Train multiple models (RF, XGBoost, LightGBM, SVM, Logistic Regression)\n",
        "- Cross-validation and performance metrics\n",
        "- SHAP analysis for interpretability\n",
        "- ROC curves and confusion matrices\n",
        "- Save best model and results\n",
        "\n",
        "## Models:\n",
        "- Random Forest (Regularized)\n",
        "- XGBoost (Regularized) \n",
        "- LightGBM (Regularized)\n",
        "- SVM (Regularized)\n",
        "- Logistic Regression (L1/L2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Colab path detection and setup\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    # Find project root (where src/ folder exists)\n",
        "    possible_paths = [\n",
        "        '/content/Alzheimer-s/Alzheimer-s',  # Nested after git clone\n",
        "        '/content/Alzheimer-s',  # Direct clone\n",
        "        '/content/Alzheimer-s/Alzhemiers',  # Alternative name\n",
        "        '.',  # Current directory\n",
        "    ]\n",
        "    \n",
        "    project_root = None\n",
        "    for path in possible_paths:\n",
        "        abs_path = os.path.abspath(path)\n",
        "        if os.path.exists(os.path.join(abs_path, 'src')):\n",
        "            # Check if src/interpretability.py exists and has our functions\n",
        "            interpretability_path = os.path.join(abs_path, 'src', 'interpretability.py')\n",
        "            if os.path.exists(interpretability_path):\n",
        "                # Check if file has our wrapper functions\n",
        "                with open(interpretability_path, 'r') as f:\n",
        "                    content = f.read()\n",
        "                    if 'def plot_shap_summary' in content and 'def plot_feature_importance' in content:\n",
        "                        project_root = abs_path\n",
        "                        break\n",
        "    \n",
        "    if project_root:\n",
        "        os.chdir(project_root)\n",
        "        print(f\"‚úÖ Changed working directory to: {project_root}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Could not find project root with src/interpretability.py\")\n",
        "        \n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Add src to path (try multiple ways)\n",
        "current_dir = os.getcwd()\n",
        "project_added = False\n",
        "\n",
        "if os.path.exists('src'):\n",
        "    sys.path.insert(0, current_dir)\n",
        "    sys.path.insert(0, os.path.join(current_dir, 'src'))\n",
        "    project_added = True\n",
        "    print(f\"‚úÖ Added to path: {current_dir} and {os.path.join(current_dir, 'src')}\")\n",
        "\n",
        "# Also check parent directories\n",
        "if not project_added:\n",
        "    for parent in ['..', '../..']:\n",
        "        parent_path = os.path.abspath(parent)\n",
        "        if os.path.exists(os.path.join(parent_path, 'src')):\n",
        "            interpretability_path = os.path.join(parent_path, 'src', 'interpretability.py')\n",
        "            if os.path.exists(interpretability_path):\n",
        "                sys.path.insert(0, parent_path)\n",
        "                sys.path.insert(0, os.path.join(parent_path, 'src'))\n",
        "                print(f\"‚úÖ Added to path: {parent_path} and {os.path.join(parent_path, 'src')}\")\n",
        "                project_added = True\n",
        "                break\n",
        "\n",
        "print(f\"üìÇ Current directory: {os.getcwd()}\")\n",
        "print(f\"üìÇ Python path includes: {sys.path[:5]}\")\n",
        "\n",
        "# Set thread limits for stability\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['NUMEXPR_MAX_THREADS'] = '1'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Import our modules with proper path handling\n",
        "try:\n",
        "    # First, find the correct src/interpretability.py file\n",
        "    correct_src_path = None\n",
        "    correct_project_root = None\n",
        "    \n",
        "    # Check current directory and sys.path\n",
        "    paths_to_check = [current_dir] + sys.path + [os.path.abspath('..'), os.path.abspath('../..')]\n",
        "    \n",
        "    for path in paths_to_check:\n",
        "        if not path or not os.path.exists(path):\n",
        "            continue\n",
        "            \n",
        "        # Check if src/interpretability.py exists here\n",
        "        test_path = os.path.join(path, 'src', 'interpretability.py')\n",
        "        if os.path.exists(test_path):\n",
        "            with open(test_path, 'r') as f:\n",
        "                content = f.read()\n",
        "                if 'def plot_shap_summary' in content and 'def plot_feature_importance' in content:\n",
        "                    correct_src_path = os.path.join(path, 'src')\n",
        "                    correct_project_root = path\n",
        "                    break\n",
        "    \n",
        "    # If we found the correct file, ensure its directory is first in sys.path\n",
        "    if correct_src_path and correct_project_root:\n",
        "        # Clear sys.path and rebuild with correct paths first\n",
        "        import copy\n",
        "        original_path = copy.copy(sys.path)\n",
        "        sys.path.clear()\n",
        "        \n",
        "        # Add correct paths first\n",
        "        sys.path.insert(0, correct_src_path)\n",
        "        sys.path.insert(0, correct_project_root)\n",
        "        \n",
        "        # Add back other paths that don't conflict\n",
        "        for p in original_path:\n",
        "            if p not in [correct_src_path, correct_project_root]:\n",
        "                # Check if this path has an old interpretability.py\n",
        "                test_old = os.path.join(p, 'src', 'interpretability.py')\n",
        "                if os.path.exists(test_old):\n",
        "                    with open(test_old, 'r') as f:\n",
        "                        content = f.read()\n",
        "                        if 'def plot_shap_summary' not in content:\n",
        "                            continue  # Skip old versions\n",
        "                sys.path.append(p)\n",
        "        \n",
        "        print(f\"‚úÖ Prioritized correct path: {correct_project_root}\")\n",
        "        print(f\"‚úÖ Using src from: {correct_src_path}\")\n",
        "    \n",
        "    # Remove any cached module\n",
        "    import importlib\n",
        "    modules_to_remove = [k for k in list(sys.modules.keys()) if k.startswith('src.interpretability')]\n",
        "    for mod_name in modules_to_remove:\n",
        "        del sys.modules[mod_name]\n",
        "        print(f\"  Removed cached module: {mod_name}\")\n",
        "    \n",
        "    # Now try importing\n",
        "    from src.model import get_models, train_and_eval\n",
        "    from src.utils import load_data, split_data, ensure_dirs, save_artifacts, plot_roc_curves, plot_confusion\n",
        "    from src.interpretability import plot_shap_summary, plot_feature_importance\n",
        "    \n",
        "    print(\"‚úÖ Successfully imported src modules\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import error: {e}\")\n",
        "    print(\"üí° Checking interpretability.py location...\")\n",
        "    \n",
        "    # Debug: find where interpretability.py actually is\n",
        "    for path in sys.path:\n",
        "        interpretability_path = os.path.join(path, 'src', 'interpretability.py')\n",
        "        if os.path.exists(interpretability_path):\n",
        "            print(f\"  Found at: {interpretability_path}\")\n",
        "            # Check if it has our functions\n",
        "            with open(interpretability_path, 'r') as f:\n",
        "                content = f.read()\n",
        "                has_shap = 'def plot_shap_summary' in content\n",
        "                has_feat = 'def plot_feature_importance' in content\n",
        "                print(f\"  Has plot_shap_summary: {has_shap}\")\n",
        "                print(f\"  Has plot_feature_importance: {has_feat}\")\n",
        "    \n",
        "    print(\"üí° Trying direct imports...\")\n",
        "    # Fallback: define minimal functions inline\n",
        "    def get_models(random_state=42):\n",
        "        return {}\n",
        "    def train_and_eval(*args, **kwargs):\n",
        "        return {}\n",
        "    def ensure_dirs(path):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "    print(\"‚ö†Ô∏è Using fallback functions - some features may be limited\")\n",
        "\n",
        "# Create results directory\n",
        "ensure_dirs('results')\n",
        "\n",
        "print(\"‚úÖ Setup complete - Ready for model training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Override models for Colab with larger training values (n_estimators=1000, max_iter=1000)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "COLAB_LARGE_TRAINING = True\n",
        "\n",
        "models_large = {\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=1000,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        max_features='sqrt',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'XGBoost': xgb.XGBClassifier(\n",
        "        n_estimators=1000,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.0,\n",
        "        reg_lambda=1.0,\n",
        "        eval_metric='logloss',\n",
        "        use_label_encoder=False,\n",
        "        verbosity=0,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'LightGBM': lgb.LGBMClassifier(\n",
        "        n_estimators=1000,\n",
        "        max_depth=-1,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.0,\n",
        "        reg_lambda=1.0,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'SVM': SVC(\n",
        "        C=1.0,\n",
        "        kernel='rbf',\n",
        "        gamma='scale',\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        C=1.0,\n",
        "        penalty='l2',\n",
        "        solver='lbfgs',\n",
        "        multi_class='ovr',\n",
        "        random_state=42\n",
        "    ),\n",
        "    'MLP': MLPClassifier(\n",
        "        hidden_layer_sizes=(128, 64),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        alpha=0.0001,\n",
        "        learning_rate='adaptive',\n",
        "        max_iter=1000,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# Monkey-patch get_models to return the large config if desired\n",
        "if COLAB_LARGE_TRAINING:\n",
        "    def get_models(random_state=42):\n",
        "        return models_large\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data\n",
        "print(\"üìä Loading preprocessed data...\")\n",
        "\n",
        "# Try to load NPZ data first\n",
        "try:\n",
        "    data = np.load('data/processed/preprocessed_alz_data.npz', allow_pickle=True)\n",
        "    X_train = data['X_train']\n",
        "    X_test = data['X_test']\n",
        "    y_train = data['y_train']\n",
        "    y_test = data['y_test']\n",
        "    \n",
        "    # Handle multi-dimensional y\n",
        "    if len(y_train.shape) > 1:\n",
        "        if y_train.shape[1] == 1:\n",
        "            y_train = y_train.ravel()\n",
        "            y_test = y_test.ravel()\n",
        "        else:\n",
        "            y_train = np.argmax(y_train, axis=1)\n",
        "            y_test = np.argmax(y_test, axis=1)\n",
        "    \n",
        "    print(f\"‚úÖ Loaded NPZ data: Train {X_train.shape}, Test {X_test.shape}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è NPZ loading failed: {e}\")\n",
        "    print(\"üîÑ Loading CSV fallback...\")\n",
        "    \n",
        "    # Fallback to CSV\n",
        "    try:\n",
        "        df = load_data('data/processed/alz_clean.csv')\n",
        "        X, y = split_data(df, 'Phenotype-derived')\n",
        "        X_train, X_test, y_train, y_test = X[0], X[1], y[0], y[1]\n",
        "        print(f\"‚úÖ Loaded CSV data: Train {X_train.shape}, Test {X_test.shape}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå CSV loading failed: {e2}\")\n",
        "        print(\"üîÑ Creating sample data...\")\n",
        "        \n",
        "        # Create sample data\n",
        "        np.random.seed(42)\n",
        "        X_train = np.random.randn(1000, 50)\n",
        "        X_test = np.random.randn(200, 50)\n",
        "        y_train = np.random.choice([0, 1, 2], 1000)\n",
        "        y_test = np.random.choice([0, 1, 2], 200)\n",
        "        print(f\"‚úÖ Created sample data: Train {X_train.shape}, Test {X_test.shape}\")\n",
        "\n",
        "print(f\"üìä Target distribution: {np.bincount(y_train)}\")\n",
        "print(f\"üìä Classes: {len(np.unique(y_train))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Models\n",
        "print(\"ü§ñ Training baseline models...\")\n",
        "\n",
        "# Get models\n",
        "try:\n",
        "    models = get_models(random_state=42)\n",
        "    print(f\"‚úÖ Got {len(models)} models from get_models()\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è get_models() failed: {e}\")\n",
        "    print(\"üîÑ Using models_large from previous cell...\")\n",
        "    try:\n",
        "        models = models_large\n",
        "        print(f\"‚úÖ Using {len(models)} models from models_large\")\n",
        "    except:\n",
        "        print(\"‚ùå models_large not available\")\n",
        "        models = {}\n",
        "\n",
        "print(f\"üìä Models to train: {list(models.keys())}\")\n",
        "\n",
        "# Train and evaluate\n",
        "results = {}\n",
        "if not models:\n",
        "    print(\"‚ùå No models available to train!\")\n",
        "else:\n",
        "    # Use train_and_eval (now fixed with proper error handling)\n",
        "    try:\n",
        "        results = train_and_eval(models, X_train, y_train, X_test, y_test, cv_folds=3)\n",
        "        print(f\"\\n‚úÖ train_and_eval completed, got {len(results)} results\")\n",
        "        \n",
        "        # Fallback to inline training ONLY if train_and_eval returned empty\n",
        "        if len(results) == 0:\n",
        "            print(\"‚ö†Ô∏è train_and_eval returned empty results, using inline training fallback...\")\n",
        "            from sklearn.model_selection import cross_val_score\n",
        "            from sklearn.metrics import accuracy_score\n",
        "            \n",
        "            for name, model in models.items():\n",
        "                print(f\"  Training {name} (fallback)...\")\n",
        "                try:\n",
        "                    model.fit(X_train, y_train)\n",
        "                    y_pred = model.predict(X_test)\n",
        "                    acc = accuracy_score(y_test, y_pred)\n",
        "                    cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "                    \n",
        "                    results[name] = {\n",
        "                        'model': model,\n",
        "                        'pred': y_pred,\n",
        "                        'accuracy': acc,\n",
        "                        'cv_mean': cv_scores.mean(),\n",
        "                        'cv_std': cv_scores.std()\n",
        "                    }\n",
        "                    print(f\"    ‚úÖ {name}: Accuracy={acc:.4f}, CV={cv_scores.mean():.4f}¬±{cv_scores.std():.4f}\")\n",
        "                except Exception as e2:\n",
        "                    print(f\"    ‚ùå {name} failed: {e2}\")\n",
        "                    continue\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå train_and_eval crashed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        results = {}\n",
        "\n",
        "print(f\"\\nüìä Total results collected: {len(results)}\")\n",
        "print(f\"üìä Results keys: {list(results.keys())}\")\n",
        "\n",
        "print(\"\\nüìä Model Performance Summary:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create summary DataFrame\n",
        "if results:\n",
        "    summary_data = []\n",
        "    for name, res in results.items():\n",
        "        if isinstance(res, dict) and 'accuracy' in res:\n",
        "            summary_data.append({\n",
        "                'Model': name,\n",
        "                'Accuracy': res['accuracy'],\n",
        "                'CV_Mean': res['cv_mean'],\n",
        "                'CV_Std': res['cv_std']\n",
        "            })\n",
        "    \n",
        "    if summary_data:\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        summary_df = summary_df.sort_values('Accuracy', ascending=False)\n",
        "        print(summary_df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No valid results to display\")\n",
        "        summary_df = pd.DataFrame()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No models were trained successfully\")\n",
        "    summary_df = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "if not summary_df.empty and len(summary_df) > 0:\n",
        "    print(\"üìà Generating visualizations...\")\n",
        "    \n",
        "    # Accuracy comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    summary_df.plot(x='Model', y='Accuracy', kind='bar', ax=plt.gca())\n",
        "    plt.title('Model Accuracy Comparison')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylabel('Accuracy')\n",
        "    \n",
        "    # CV scores with error bars\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.errorbar(range(len(summary_df)), summary_df['CV_Mean'], \n",
        "                 yerr=summary_df['CV_Std'], fmt='o', capsize=5)\n",
        "    plt.xticks(range(len(summary_df)), summary_df['Model'], rotation=45)\n",
        "    plt.title('Cross-Validation Scores')\n",
        "    plt.ylabel('CV Score')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # ROC Curves (if function available)\n",
        "    if results and 'plot_roc_curves' in globals():\n",
        "        try:\n",
        "            print(\"\\nüìä ROC Curves:\")\n",
        "            plot_roc_curves(results, y_test)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è ROC curves failed: {e}\")\n",
        "    \n",
        "    # Confusion Matrix for best model\n",
        "    if results:\n",
        "        try:\n",
        "            best_model_name = summary_df.iloc[0]['Model']\n",
        "            if best_model_name in results and 'model' in results[best_model_name]:\n",
        "                best_model = results[best_model_name]['model']\n",
        "                best_pred = results[best_model_name]['pred']\n",
        "                \n",
        "                if 'plot_confusion' in globals():\n",
        "                    print(f\"\\nüìä Confusion Matrix - {best_model_name}:\")\n",
        "                    plot_confusion(y_test, best_pred, normalize=True)\n",
        "                else:\n",
        "                    # Fallback confusion matrix\n",
        "                    from sklearn.metrics import confusion_matrix\n",
        "                    cm = confusion_matrix(y_test, best_pred)\n",
        "                    plt.figure(figsize=(8, 6))\n",
        "                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "                    plt.title(f'Confusion Matrix - {best_model_name}')\n",
        "                    plt.xlabel('Predicted')\n",
        "                    plt.ylabel('True')\n",
        "                    plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Confusion matrix failed: {e}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results available for visualization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHAP Analysis (for tree-based models)\n",
        "print(\"üîç SHAP Analysis for interpretability...\")\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    \n",
        "    # Analyze tree-based models\n",
        "    tree_models = ['Random Forest', 'XGBoost', 'LightGBM']\n",
        "    \n",
        "    for model_name in tree_models:\n",
        "        if model_name in results:\n",
        "            print(f\"\\nüîç Analyzing {model_name}...\")\n",
        "            \n",
        "            model = results[model_name]['model']\n",
        "            \n",
        "            # Create SHAP explainer\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                explainer = shap.TreeExplainer(model)\n",
        "                shap_values = explainer.shap_values(X_test[:100])  # Sample for speed\n",
        "                \n",
        "                # Summary plot\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                shap.summary_plot(shap_values, X_test[:100], show=False)\n",
        "                plt.title(f'SHAP Summary - {model_name}')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "                \n",
        "                print(f\"‚úÖ SHAP analysis complete for {model_name}\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è {model_name} doesn't support SHAP analysis\")\n",
        "                \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è SHAP not installed. Install with: pip install shap\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è SHAP analysis failed: {e}\")\n",
        "\n",
        "# Feature Importance for tree models\n",
        "print(\"\\nüìä Feature Importance Analysis:\")\n",
        "for model_name in ['Random Forest', 'XGBoost', 'LightGBM']:\n",
        "    if model_name in results:\n",
        "        plot_feature_importance(results[model_name]['model'], model_name, top_n=20, X_train=X_train, y_train=y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Results\n",
        "if not summary_df.empty and len(results) > 0:\n",
        "    print(\"üíæ Saving results and best model...\")\n",
        "    \n",
        "    # Save metrics CSV\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_path = f'results/model_summary_{timestamp}.csv'\n",
        "    summary_df.to_csv(csv_path, index=False)\n",
        "    print(f\"‚úÖ Metrics saved to: {csv_path}\")\n",
        "    \n",
        "    # Save best model\n",
        "    try:\n",
        "        if len(summary_df) > 0 and not summary_df.empty:\n",
        "            best_model_name = summary_df.iloc[0]['Model']\n",
        "            if best_model_name in results and 'model' in results[best_model_name]:\n",
        "                best_model = results[best_model_name]['model']\n",
        "                model_path = f'results/best_model_{timestamp}.pkl'\n",
        "                \n",
        "                import joblib\n",
        "                joblib.dump(best_model, model_path)\n",
        "                print(f\"‚úÖ Best model ({best_model_name}) saved to: {model_path}\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Best model '{best_model_name}' not found in results\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è summary_df is empty, cannot save best model\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not save best model: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    # Save detailed results JSON\n",
        "    try:\n",
        "        import json\n",
        "        detailed_results = {}\n",
        "        for name, res in results.items():\n",
        "            if isinstance(res, dict) and 'accuracy' in res:\n",
        "                detailed_results[name] = {\n",
        "                    'accuracy': float(res['accuracy']),\n",
        "                    'cv_mean': float(res['cv_mean']),\n",
        "                    'cv_std': float(res['cv_std'])\n",
        "                }\n",
        "        \n",
        "        if detailed_results:\n",
        "            json_path = f'results/detailed_results_{timestamp}.json'\n",
        "            with open(json_path, 'w') as f:\n",
        "                json.dump(detailed_results, f, indent=2)\n",
        "            print(f\"‚úÖ Detailed results saved to: {json_path}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No valid results to save in JSON\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not save JSON: {e}\")\n",
        "    \n",
        "    print(f\"\\nüéâ Training complete!\")\n",
        "    if len(summary_df) > 0 and not summary_df.empty:\n",
        "        print(f\"üèÜ Best model: {summary_df.iloc[0]['Model']} (Accuracy: {summary_df.iloc[0]['Accuracy']:.4f})\")\n",
        "    print(f\"üìä All results saved to results/ directory\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results to save\")\n",
        "    print(f\"Debug: summary_df.empty = {summary_df.empty if 'summary_df' in locals() else 'not defined'}\")\n",
        "    print(f\"Debug: len(results) = {len(results) if 'results' in locals() else 'not defined'}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
