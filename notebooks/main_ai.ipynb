{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab setup (safe to run locally; does nothing if already installed)\n",
        "%pip install -q xgboost lightgbm shap pyarrow category_encoders scikit-learn matplotlib seaborn joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Core AI Training for Alzheimer's Disease Prediction\n",
        "\n",
        "This notebook trains baseline machine learning models on preprocessed genomic data and provides comprehensive evaluation with interpretability analysis.\n",
        "\n",
        "## Features:\n",
        "- Load preprocessed NPZ data or fallback to CSV\n",
        "- Train multiple models (RF, XGBoost, LightGBM, SVM, Logistic Regression)\n",
        "- Cross-validation and performance metrics\n",
        "- SHAP analysis for interpretability\n",
        "- ROC curves and confusion matrices\n",
        "- Save best model and results\n",
        "\n",
        "## Models:\n",
        "- Random Forest (Regularized)\n",
        "- XGBoost (Regularized) \n",
        "- LightGBM (Regularized)\n",
        "- SVM (Regularized)\n",
        "- Logistic Regression (L1/L2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('./src')\n",
        "\n",
        "# Set thread limits for stability\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "os.environ['NUMEXPR_MAX_THREADS'] = '1'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Import our modules\n",
        "from src.model import get_models, train_and_eval\n",
        "from src.utils import load_data, split_data, ensure_dirs, save_artifacts, plot_roc_curves, plot_confusion\n",
        "from src.interpretability import plot_shap_summary, plot_feature_importance\n",
        "\n",
        "# Create results directory\n",
        "ensure_dirs('results')\n",
        "\n",
        "print(\"‚úÖ Setup complete - Ready for model training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Override models for Colab with larger training values (n_estimators=1000, max_iter=1000)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "COLAB_LARGE_TRAINING = True\n",
        "\n",
        "models_large = {\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=1000,\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        max_features='sqrt',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'XGBoost': xgb.XGBClassifier(\n",
        "        n_estimators=1000,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.0,\n",
        "        reg_lambda=1.0,\n",
        "        eval_metric='logloss',\n",
        "        use_label_encoder=False,\n",
        "        verbosity=0,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'LightGBM': lgb.LGBMClassifier(\n",
        "        n_estimators=1000,\n",
        "        max_depth=-1,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.0,\n",
        "        reg_lambda=1.0,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'SVM': SVC(\n",
        "        C=1.0,\n",
        "        kernel='rbf',\n",
        "        gamma='scale',\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        C=1.0,\n",
        "        penalty='l2',\n",
        "        solver='lbfgs',\n",
        "        multi_class='ovr',\n",
        "        random_state=42\n",
        "    ),\n",
        "    'MLP': MLPClassifier(\n",
        "        hidden_layer_sizes=(128, 64),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        alpha=0.0001,\n",
        "        learning_rate='adaptive',\n",
        "        max_iter=1000,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# Monkey-patch get_models to return the large config if desired\n",
        "if COLAB_LARGE_TRAINING:\n",
        "    def get_models(random_state=42):\n",
        "        return models_large\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data\n",
        "print(\"üìä Loading preprocessed data...\")\n",
        "\n",
        "# Try to load NPZ data first\n",
        "try:\n",
        "    data = np.load('data/processed/preprocessed_alz_data.npz', allow_pickle=True)\n",
        "    X_train = data['X_train']\n",
        "    X_test = data['X_test']\n",
        "    y_train = data['y_train']\n",
        "    y_test = data['y_test']\n",
        "    \n",
        "    # Handle multi-dimensional y\n",
        "    if len(y_train.shape) > 1:\n",
        "        if y_train.shape[1] == 1:\n",
        "            y_train = y_train.ravel()\n",
        "            y_test = y_test.ravel()\n",
        "        else:\n",
        "            y_train = np.argmax(y_train, axis=1)\n",
        "            y_test = np.argmax(y_test, axis=1)\n",
        "    \n",
        "    print(f\"‚úÖ Loaded NPZ data: Train {X_train.shape}, Test {X_test.shape}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è NPZ loading failed: {e}\")\n",
        "    print(\"üîÑ Loading CSV fallback...\")\n",
        "    \n",
        "    # Fallback to CSV\n",
        "    try:\n",
        "        df = load_data('data/processed/alz_clean.csv')\n",
        "        X, y = split_data(df, 'Phenotype-derived')\n",
        "        X_train, X_test, y_train, y_test = X[0], X[1], y[0], y[1]\n",
        "        print(f\"‚úÖ Loaded CSV data: Train {X_train.shape}, Test {X_test.shape}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå CSV loading failed: {e2}\")\n",
        "        print(\"üîÑ Creating sample data...\")\n",
        "        \n",
        "        # Create sample data\n",
        "        np.random.seed(42)\n",
        "        X_train = np.random.randn(1000, 50)\n",
        "        X_test = np.random.randn(200, 50)\n",
        "        y_train = np.random.choice([0, 1, 2], 1000)\n",
        "        y_test = np.random.choice([0, 1, 2], 200)\n",
        "        print(f\"‚úÖ Created sample data: Train {X_train.shape}, Test {X_test.shape}\")\n",
        "\n",
        "print(f\"üìä Target distribution: {np.bincount(y_train)}\")\n",
        "print(f\"üìä Classes: {len(np.unique(y_train))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Models\n",
        "print(\"ü§ñ Training baseline models...\")\n",
        "\n",
        "# Get models\n",
        "models = get_models(random_state=42)\n",
        "\n",
        "# Train and evaluate\n",
        "results = train_and_eval(models, X_train, y_train, X_test, y_test, cv_folds=3)\n",
        "\n",
        "print(\"\\nüìä Model Performance Summary:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_data = []\n",
        "for name, res in results.items():\n",
        "    summary_data.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': res['accuracy'],\n",
        "        'CV_Mean': res['cv_mean'],\n",
        "        'CV_Std': res['cv_std']\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df = summary_df.sort_values('Accuracy', ascending=False)\n",
        "print(summary_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "print(\"üìà Generating visualizations...\")\n",
        "\n",
        "# Accuracy comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "summary_df.plot(x='Model', y='Accuracy', kind='bar', ax=plt.gca())\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# CV scores with error bars\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.errorbar(range(len(summary_df)), summary_df['CV_Mean'], \n",
        "             yerr=summary_df['CV_Std'], fmt='o', capsize=5)\n",
        "plt.xticks(range(len(summary_df)), summary_df['Model'], rotation=45)\n",
        "plt.title('Cross-Validation Scores')\n",
        "plt.ylabel('CV Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ROC Curves\n",
        "print(\"\\nüìä ROC Curves:\")\n",
        "plot_roc_curves(results, y_test)\n",
        "\n",
        "# Confusion Matrix for best model\n",
        "best_model_name = summary_df.iloc[0]['Model']\n",
        "best_model = results[best_model_name]['model']\n",
        "best_pred = results[best_model_name]['pred']\n",
        "\n",
        "print(f\"\\nüìä Confusion Matrix - {best_model_name}:\")\n",
        "plot_confusion(y_test, best_pred, normalize=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHAP Analysis (for tree-based models)\n",
        "print(\"üîç SHAP Analysis for interpretability...\")\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    \n",
        "    # Analyze tree-based models\n",
        "    tree_models = ['Random Forest', 'XGBoost', 'LightGBM']\n",
        "    \n",
        "    for model_name in tree_models:\n",
        "        if model_name in results:\n",
        "            print(f\"\\nüîç Analyzing {model_name}...\")\n",
        "            \n",
        "            model = results[model_name]['model']\n",
        "            \n",
        "            # Create SHAP explainer\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                explainer = shap.TreeExplainer(model)\n",
        "                shap_values = explainer.shap_values(X_test[:100])  # Sample for speed\n",
        "                \n",
        "                # Summary plot\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                shap.summary_plot(shap_values, X_test[:100], show=False)\n",
        "                plt.title(f'SHAP Summary - {model_name}')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "                \n",
        "                print(f\"‚úÖ SHAP analysis complete for {model_name}\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è {model_name} doesn't support SHAP analysis\")\n",
        "                \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è SHAP not installed. Install with: pip install shap\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è SHAP analysis failed: {e}\")\n",
        "\n",
        "# Feature Importance for tree models\n",
        "print(\"\\nüìä Feature Importance Analysis:\")\n",
        "for model_name in ['Random Forest', 'XGBoost', 'LightGBM']:\n",
        "    if model_name in results:\n",
        "        plot_feature_importance(results[model_name]['model'], model_name, top_n=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Results\n",
        "print(\"üíæ Saving results and best model...\")\n",
        "\n",
        "# Save metrics CSV\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_path = f'results/model_summary_{timestamp}.csv'\n",
        "summary_df.to_csv(csv_path, index=False)\n",
        "print(f\"‚úÖ Metrics saved to: {csv_path}\")\n",
        "\n",
        "# Save best model\n",
        "best_model_name = summary_df.iloc[0]['Model']\n",
        "best_model = results[best_model_name]['model']\n",
        "model_path = f'results/best_model_{timestamp}.pkl'\n",
        "\n",
        "import joblib\n",
        "joblib.dump(best_model, model_path)\n",
        "print(f\"‚úÖ Best model ({best_model_name}) saved to: {model_path}\")\n",
        "\n",
        "# Save detailed results JSON\n",
        "import json\n",
        "detailed_results = {}\n",
        "for name, res in results.items():\n",
        "    detailed_results[name] = {\n",
        "        'accuracy': float(res['accuracy']),\n",
        "        'cv_mean': float(res['cv_mean']),\n",
        "        'cv_std': float(res['cv_std'])\n",
        "    }\n",
        "\n",
        "json_path = f'results/detailed_results_{timestamp}.json'\n",
        "with open(json_path, 'w') as f:\n",
        "    json.dump(detailed_results, f, indent=2)\n",
        "print(f\"‚úÖ Detailed results saved to: {json_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Training complete!\")\n",
        "print(f\"üèÜ Best model: {best_model_name} (Accuracy: {summary_df.iloc[0]['Accuracy']:.4f})\")\n",
        "print(f\"üìä All results saved to results/ directory\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
