{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udde0 Alzheimer's Disease Prediction - Colab Training Notebook\n",
        "\n",
        "This notebook provides a complete training pipeline for Alzheimer's Disease prediction with:\n",
        "- Data loading with automatic fallback\n",
        "- Runtime validation (GPU detection, version checks)\n",
        "- Multiple ML model training\n",
        "- Bootstrap confidence intervals\n",
        "\n",
        "## \ud83d\udccb Colab Setup Instructions\n",
        "\n",
        "1. **Restart Runtime**: Runtime \u2192 Restart runtime (or Ctrl+M \u2192 Restart runtime)\n",
        "2. **Enable GPU**: Runtime \u2192 Change runtime type \u2192 Hardware accelerator \u2192 GPU\n",
        "3. **Mount Google Drive** (optional): If using external files\n",
        "   ```python\n",
        "   from google.colab import drive\n",
        "   drive.mount('/content/drive')\n",
        "   ```\n",
        "\n",
        "4. **Install Dependencies**: Run the setup cell below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q scikit-learn xgboost lightgbm optuna numpy pandas matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd0d Runtime Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import platform\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\ud83d\udd0d Runtime Validation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Python version\n",
        "print(f\"\ud83d\udc0d Python version: {sys.version.split()[0]}\")\n",
        "print(f\"\ud83d\udcbb Platform: {platform.system()} {platform.release()}\")\n",
        "\n",
        "# GPU detection\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"\u26a1 GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"\ud83d\udcca GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    else:\n",
        "        print(\"\u26a0\ufe0f No GPU detected - using CPU\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        if tf.config.list_physical_devices('GPU'):\n",
        "            gpu = tf.config.list_physical_devices('GPU')[0]\n",
        "            print(f\"\u26a1 GPU detected: {gpu}\")\n",
        "        else:\n",
        "            print(\"\u26a0\ufe0f No GPU detected - using CPU\")\n",
        "    except ImportError:\n",
        "        print(\"\u26a0\ufe0f PyTorch/TensorFlow not installed - cannot detect GPU\")\n",
        "        print(\"\ud83d\udca1 To enable GPU: Runtime \u2192 Change runtime type \u2192 GPU\")\n",
        "\n",
        "# Package versions\n",
        "print(\"\\n\ud83d\udce6 Package Versions:\")\n",
        "packages = ['sklearn', 'xgboost', 'lightgbm', 'optuna']\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        mod = __import__(pkg)\n",
        "        version = getattr(mod, '__version__', 'unknown')\n",
        "        print(f\"  \u2705 {pkg}: {version}\")\n",
        "    except ImportError:\n",
        "        print(f\"  \u274c {pkg}: not installed\")\n",
        "\n",
        "print(\"\\n\u2705 Runtime validation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Data Loading with Fallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "print(\"\ud83d\udce6 Loading data...\")\n",
        "\n",
        "X = None\n",
        "y = None\n",
        "data_source = None\n",
        "\n",
        "# Try loading NPZ file first\n",
        "try:\n",
        "    data = np.load('preprocessed_data.npz', allow_pickle=True)\n",
        "    if 'X' in data and 'y' in data:\n",
        "        X = data['X']\n",
        "        y = data['y']\n",
        "        data_source = 'NPZ'\n",
        "        print(f\"\u2705 Data loaded from preprocessed_data.npz\")\n",
        "        print(f\"   Shape: X={X.shape}, y={y.shape}\")\n",
        "    else:\n",
        "        print(\"\u26a0\ufe0f NPZ file found but missing 'X' or 'y' keys\")\n",
        "        print(f\"   Available keys: {list(data.keys())}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\u26a0\ufe0f preprocessed_data.npz not found, trying CSV fallback...\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f Error loading NPZ: {e}\")\n",
        "    print(\"   Trying CSV fallback...\")\n",
        "\n",
        "# Fallback to CSV\n",
        "if X is None or y is None:\n",
        "    try:\n",
        "        df = pd.read_csv('fallback_data.csv')\n",
        "        print(f\"\u2705 Data loaded from fallback_data.csv\")\n",
        "        print(f\"   Shape: {df.shape}\")\n",
        "        \n",
        "        # Assume last column is target\n",
        "        y = df.iloc[:, -1].values\n",
        "        X = df.iloc[:, :-1].values\n",
        "        data_source = 'CSV'\n",
        "        print(f\"   Extracted: X={X.shape}, y={y.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\u274c fallback_data.csv not found\")\n",
        "        print(\"\ud83d\udca1 Creating sample data for demonstration...\")\n",
        "        # Create sample data\n",
        "        np.random.seed(42)\n",
        "        X = np.random.randn(1000, 50)\n",
        "        y = np.random.choice([0, 1, 2], 1000)\n",
        "        data_source = 'SAMPLE'\n",
        "        print(f\"   Generated sample: X={X.shape}, y={y.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Error loading CSV: {e}\")\n",
        "        raise\n",
        "\n",
        "# Handle multi-dimensional y\n",
        "if len(y.shape) > 1:\n",
        "    if y.shape[1] == 1:\n",
        "        y = y.ravel()\n",
        "    else:\n",
        "        y = np.argmax(y, axis=1)\n",
        "\n",
        "# Data preprocessing\n",
        "print(f\"\\n\ud83d\udd27 Preprocessing data (source: {data_source})...\")\n",
        "\n",
        "# Handle NaN and infinity\n",
        "if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n",
        "    print(\"   Cleaning NaN and infinity values...\")\n",
        "    X = np.where(np.isinf(X), np.nan, X)\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X = imputer.fit_transform(X)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n\u2705 Data preprocessing complete\")\n",
        "print(f\"   Train: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"   Test: X={X_test.shape}, y={y_test.shape}\")\n",
        "print(f\"   Classes: {len(np.unique(y_train))} (distribution: {np.bincount(y_train)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\udd16 Model Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "print(\"\ud83e\udd16 Training Models\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'XGBoost': xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        verbosity=0\n",
        "    ),\n",
        "    'LightGBM': lgb.LGBMClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        verbose=-1\n",
        "    ),\n",
        "    'SVM': SVC(\n",
        "        kernel='rbf',\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n\ud83d\udd01 Training {name}...\")\n",
        "    \n",
        "    try:\n",
        "        # Train\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Predict\n",
        "        y_pred = model.predict(X_test)\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        \n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'predictions': y_pred\n",
        "        }\n",
        "        \n",
        "        print(f\"   \u2705 Accuracy: {accuracy:.4f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   \u274c Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"\ud83d\udcca Model Performance Summary:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Sort by accuracy\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
        "\n",
        "for name, res in sorted_results:\n",
        "    print(f\"{name:20s}: {res['accuracy']:.4f}\")\n",
        "\n",
        "if sorted_results:\n",
        "    best_name, best_result = sorted_results[0]\n",
        "    print(f\"\\n\ud83c\udfc6 Best Model: {best_name} (Accuracy: {best_result['accuracy']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcc8 Bootstrap Confidence Intervals (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bootstrap_confidence_interval(model, X_test, y_test, n_bootstrap=300, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Calculate bootstrap confidence intervals for model accuracy.\n",
        "    \"\"\"\n",
        "    n_samples = len(y_test)\n",
        "    accuracies = []\n",
        "    \n",
        "    print(f\"\ud83d\udd04 Running {n_bootstrap} bootstrap iterations...\")\n",
        "    \n",
        "    for i in range(n_bootstrap):\n",
        "        # Bootstrap sample\n",
        "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        X_boot = X_test[indices]\n",
        "        y_boot = y_test[indices]\n",
        "        \n",
        "        # Predict\n",
        "        y_pred = model.predict(X_boot)\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        acc = accuracy_score(y_boot, y_pred)\n",
        "        accuracies.append(acc)\n",
        "        \n",
        "        # Progress indicator\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"   Completed {i + 1}/{n_bootstrap} iterations\")\n",
        "    \n",
        "    # Calculate statistics\n",
        "    accuracies = np.array(accuracies)\n",
        "    mean_acc = np.mean(accuracies)\n",
        "    std_acc = np.std(accuracies)\n",
        "    \n",
        "    # Calculate confidence interval\n",
        "    alpha = 1 - confidence\n",
        "    lower = np.percentile(accuracies, 100 * alpha / 2)\n",
        "    upper = np.percentile(accuracies, 100 * (1 - alpha / 2))\n",
        "    \n",
        "    return {\n",
        "        'mean': mean_acc,\n",
        "        'std': std_acc,\n",
        "        'lower': lower,\n",
        "        'upper': upper,\n",
        "        'confidence': confidence\n",
        "    }\n",
        "\n",
        "# Run bootstrap CI for top models\n",
        "if results:\n",
        "    print(\"\\n\ud83d\udcca Bootstrap Confidence Intervals for Top Models\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Get top 3 models\n",
        "    top_models = sorted_results[:3]\n",
        "    \n",
        "    bootstrap_results = {}\n",
        "    \n",
        "    for name, res in top_models:\n",
        "        print(f\"\\n\ud83d\udd0d Analyzing {name}...\")\n",
        "        ci = bootstrap_confidence_interval(res['model'], X_test, y_test, n_bootstrap=300)\n",
        "        bootstrap_results[name] = ci\n",
        "        \n",
        "        print(f\"   Mean Accuracy: {ci['mean']:.4f} \u00b1 {ci['std']:.4f}\")\n",
        "        print(f\"   {int(ci['confidence']*100)}% CI: [{ci['lower']:.4f}, {ci['upper']:.4f}]\")\n",
        "    \n",
        "    print(\"\\n\u2705 Bootstrap analysis complete\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f No trained models available for bootstrap analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcbe Save Results (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Save best model\n",
        "if results:\n",
        "    best_name, best_result = sorted_results[0]\n",
        "    \n",
        "    # Save model\n",
        "    model_filename = f'best_model_{best_name.replace(\" \", \"_\")}.pkl'\n",
        "    joblib.dump(best_result['model'], model_filename)\n",
        "    print(f\"\ud83d\udcbe Saved best model: {model_filename}\")\n",
        "    \n",
        "    # Save results summary\n",
        "    summary = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'best_model': best_name,\n",
        "        'best_accuracy': float(best_result['accuracy']),\n",
        "        'all_results': {name: float(res['accuracy']) for name, res in results.items()}\n",
        "    }\n",
        "    \n",
        "    if 'bootstrap_results' in locals():\n",
        "        summary['bootstrap'] = {\n",
        "            name: {k: float(v) for k, v in ci.items() if k != 'confidence'}\n",
        "            for name, ci in bootstrap_results.items()\n",
        "        }\n",
        "    \n",
        "    summary_filename = 'training_results.json'\n",
        "    with open(summary_filename, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    \n",
        "    print(f\"\ud83d\udcbe Saved results summary: {summary_filename}\")\n",
        "    print(\"\\n\ud83d\udcc1 Files saved to current directory\")\n",
        "    print(\"   \ud83d\udca1 To download: Right-click file \u2192 Download\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f No results to save\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcdd Notes\n",
        "\n",
        "- **Runtime Management**: If you encounter memory issues, restart the runtime (Runtime \u2192 Restart runtime)\n",
        "- **GPU Usage**: To enable GPU acceleration, go to Runtime \u2192 Change runtime type \u2192 Hardware accelerator \u2192 GPU\n",
        "- **Data Upload**: Upload `preprocessed_data.npz` or `fallback_data.csv` to the Colab file system using the file browser\n",
        "- **Download Results**: Right-click on saved files in the file browser to download them\n",
        "- **Long Training**: For long training sessions, consider using Colab Pro for longer runtime sessions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}