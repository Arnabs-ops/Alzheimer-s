{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Alzheimer's Disease Prediction - Colab Training Notebook\n",
        "\n",
        "This notebook provides a complete training pipeline for Alzheimer's Disease prediction with:\n",
        "- Data loading with automatic fallback\n",
        "- Runtime validation (GPU detection, version checks)\n",
        "- Multiple ML model training\n",
        "- Bootstrap confidence intervals\n",
        "\n",
        "## üìã Colab Setup Instructions\n",
        "\n",
        "1. **Restart Runtime**: Runtime ‚Üí Restart runtime (or Ctrl+M ‚Üí Restart runtime)\n",
        "2. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\n",
        "3. **Mount Google Drive** (optional): If using external files\n",
        "   ```python\n",
        "   from google.colab import drive\n",
        "   drive.mount('/content/drive')\n",
        "   ```\n",
        "\n",
        "4. **Clone Repository**: Run the repository setup cell to get all code from the repo\n",
        "5. **Install Dependencies**: Run the setup cell below\n",
        "6. **Upload Data** (optional): Upload your `preprocessed_data.npz` or `fallback_data.csv` file to Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• Repository Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Clone repository (Colab only - skip if running locally)\n",
        "try:\n",
        "    import google.colab\n",
        "    import os\n",
        "    \n",
        "    # Check if directory already exists\n",
        "    if os.path.exists('Alzheimer-s'):\n",
        "        print(\"üìÅ Directory already exists, pulling latest changes...\")\n",
        "        %cd Alzheimer-s\n",
        "        !git pull origin main\n",
        "        print(\"‚úÖ Pulled latest changes\")\n",
        "    else:\n",
        "        !git clone https://github.com/Arnabs-ops/Alzheimer-s.git\n",
        "        %cd Alzheimer-s\n",
        "        print(\"‚úÖ Repository cloned and directory changed\")\n",
        "except ImportError:\n",
        "    print(\"‚ÑπÔ∏è Running locally - skipping git clone\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Git operation failed: {e}\")\n",
        "    print(\"üí° Trying to continue with existing directory...\")\n",
        "    if os.path.exists('Alzheimer-s'):\n",
        "        %cd Alzheimer-s\n",
        "        print(\"‚úÖ Changed to existing directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q scikit-learn xgboost lightgbm optuna numpy pandas matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Runtime Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import platform\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üîç Runtime Validation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Python version\n",
        "print(f\"üêç Python version: {sys.version.split()[0]}\")\n",
        "print(f\"üíª Platform: {platform.system()} {platform.release()}\")\n",
        "\n",
        "# GPU detection\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"‚ö° GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"üìä GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No GPU detected - using CPU\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        if tf.config.list_physical_devices('GPU'):\n",
        "            gpu = tf.config.list_physical_devices('GPU')[0]\n",
        "            print(f\"‚ö° GPU detected: {gpu}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No GPU detected - using CPU\")\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è PyTorch/TensorFlow not installed - cannot detect GPU\")\n",
        "        print(\"üí° To enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "\n",
        "# Package versions\n",
        "print(\"\\nüì¶ Package Versions:\")\n",
        "packages = ['sklearn', 'xgboost', 'lightgbm', 'optuna']\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        mod = __import__(pkg)\n",
        "        version = getattr(mod, '__version__', 'unknown')\n",
        "        print(f\"  ‚úÖ {pkg}: {version}\")\n",
        "    except ImportError:\n",
        "        print(f\"  ‚ùå {pkg}: not installed\")\n",
        "\n",
        "print(\"\\n‚úÖ Runtime validation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Data Loading with Fallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "print(\"üì¶ Loading data...\")\n",
        "\n",
        "# Debug: List available files\n",
        "print(\"\\nüîç Checking available files...\")\n",
        "print(f\"   Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Check both current directory and /content (where Colab uploads files)\n",
        "print(f\"   Files in current directory:\")\n",
        "try:\n",
        "    current_files = [f for f in os.listdir('.') if f.endswith(('.npz', '.csv'))]\n",
        "    for f in current_files:\n",
        "        print(f\"      ‚úÖ {f}\")\n",
        "except:\n",
        "    print(\"      (none found)\")\n",
        "\n",
        "# Also check /content directory (where files are uploaded before cd)\n",
        "print(f\"   Files in /content directory:\")\n",
        "try:\n",
        "    if os.path.exists('/content'):\n",
        "        content_files = [f for f in os.listdir('/content') if f.endswith(('.npz', '.csv'))]\n",
        "        for f in content_files:\n",
        "            print(f\"      ‚úÖ /content/{f}\")\n",
        "        if content_files:\n",
        "            print(f\"   üí° Found files in /content - will check there too\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "X = None\n",
        "y = None\n",
        "data_source = None\n",
        "\n",
        "# First, check current directory for ANY .npz files\n",
        "npz_files_in_dir = []\n",
        "try:\n",
        "    npz_files_in_dir = [f for f in os.listdir('.') if f.endswith('.npz')]\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Also check /content directory (where Colab uploads files)\n",
        "npz_files_in_content = []\n",
        "try:\n",
        "    if os.path.exists('/content'):\n",
        "        npz_files_in_content = [f'/content/{f}' for f in os.listdir('/content') if f.endswith('.npz')]\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if npz_files_in_dir or npz_files_in_content:\n",
        "    total = len(npz_files_in_dir) + len(npz_files_in_content)\n",
        "    print(f\"   üìÅ Found {total} NPZ file(s):\")\n",
        "    for f in npz_files_in_dir:\n",
        "        print(f\"      - {f} (current dir)\")\n",
        "    for f in npz_files_in_content:\n",
        "        print(f\"      - {f} (/content)\")\n",
        "\n",
        "# Build list of paths to check (prioritize files in current directory, then /content)\n",
        "npz_paths = []\n",
        "# Add any .npz files found in current directory first\n",
        "npz_paths.extend(npz_files_in_dir)\n",
        "# Add files from /content\n",
        "npz_paths.extend(npz_files_in_content)\n",
        "# Then add standard paths (both in current dir and /content)\n",
        "npz_paths.extend([\n",
        "    'preprocessed_data.npz',  # Standard name (current dir)\n",
        "    'preprocessed_alz_data.npz',  # Alternative name (current dir)\n",
        "    '/content/preprocessed_data.npz',  # Standard name (/content)\n",
        "    '/content/preprocessed_alz_data.npz',  # Alternative name (/content)\n",
        "    'data/processed/preprocessed_alz_data.npz',  # Repo structure\n",
        "    'data/processed/preprocessed_data.npz',  # Alternative repo path\n",
        "])\n",
        "# Remove duplicates while preserving order\n",
        "npz_paths = list(dict.fromkeys(npz_paths))\n",
        "\n",
        "csv_paths = [\n",
        "    'fallback_data.csv',  # Current directory (uploaded)\n",
        "    'data/processed/alz_clean.csv',  # Repo structure\n",
        "    'data/processed/fallback_data.csv',  # Alternative repo path\n",
        "]\n",
        "\n",
        "# Try loading NPZ file from multiple locations\n",
        "data_file = None\n",
        "for path in npz_paths:\n",
        "    if os.path.exists(path):\n",
        "        data_file = path\n",
        "        break\n",
        "\n",
        "if data_file:\n",
        "    try:\n",
        "        data = np.load(data_file, allow_pickle=True)\n",
        "        data_keys = list(data.keys())\n",
        "        \n",
        "        # Check for X/y format\n",
        "        if 'X' in data and 'y' in data:\n",
        "            X = data['X']\n",
        "            y = data['y']\n",
        "            data_source = 'NPZ'\n",
        "            print(f\"‚úÖ Data loaded from {data_file} (X/y format)\")\n",
        "            print(f\"   Shape: X={X.shape}, y={y.shape}\")\n",
        "        # Check for train/test split format\n",
        "        elif 'X_train' in data and 'X_test' in data and 'y_train' in data and 'y_test' in data:\n",
        "            X_train = data['X_train']\n",
        "            X_test = data['X_test']\n",
        "            y_train = data['y_train']\n",
        "            y_test = data['y_test']\n",
        "            \n",
        "            # Merge train and test for initial processing\n",
        "            X = np.vstack([X_train, X_test])\n",
        "            y = np.concatenate([y_train, y_test])\n",
        "            \n",
        "            data_source = 'NPZ_SPLIT'\n",
        "            print(f\"‚úÖ Data loaded from {data_file} (train/test split format)\")\n",
        "            print(f\"   Train: X={X_train.shape}, y={y_train.shape}\")\n",
        "            print(f\"   Test: X={X_test.shape}, y={y_test.shape}\")\n",
        "            print(f\"   Combined: X={X.shape}, y={y.shape}\")\n",
        "            \n",
        "            # Skip the train_test_split later since we already have splits\n",
        "            has_existing_split = True\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è NPZ file found but missing expected keys\")\n",
        "            print(f\"   Available keys: {data_keys}\")\n",
        "            print(\"   Expected: ['X', 'y'] OR ['X_train', 'X_test', 'y_train', 'y_test']\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error loading NPZ from {data_file}: {e}\")\n",
        "        print(\"   Trying CSV fallback...\")\n",
        "        data_file = None\n",
        "        X = None\n",
        "        y = None\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è NPZ file not found in any location, trying CSV fallback...\")\n",
        "    \n",
        "has_existing_split = False if X is None or y is None else has_existing_split\n",
        "\n",
        "# Initialize split flag if not set\n",
        "if 'has_existing_split' not in locals():\n",
        "    has_existing_split = False\n",
        "\n",
        "# Fallback to CSV\n",
        "if X is None or y is None:\n",
        "    csv_file = None\n",
        "    for path in csv_paths:\n",
        "        if os.path.exists(path):\n",
        "            csv_file = path\n",
        "            break\n",
        "    \n",
        "    if csv_file:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file)\n",
        "            print(f\"‚úÖ Data loaded from {csv_file}\")\n",
        "            print(f\"   Shape: {df.shape}\")\n",
        "            \n",
        "            # Assume last column is target\n",
        "            y = df.iloc[:, -1].values\n",
        "            X = df.iloc[:, :-1].values\n",
        "            data_source = 'CSV'\n",
        "            print(f\"   Extracted: X={X.shape}, y={y.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading CSV from {csv_file}: {e}\")\n",
        "            raise\n",
        "    else:\n",
        "        print(\"‚ùå CSV file not found in any location\")\n",
        "        print(\"üí° Creating sample data for demonstration...\")\n",
        "        # Create sample data\n",
        "        np.random.seed(42)\n",
        "        X = np.random.randn(1000, 50)\n",
        "        y = np.random.choice([0, 1, 2], 1000)\n",
        "        data_source = 'SAMPLE'\n",
        "        print(f\"   Generated sample: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "# Handle multi-dimensional y\n",
        "if len(y.shape) > 1:\n",
        "    if y.shape[1] == 1:\n",
        "        y = y.ravel()\n",
        "    else:\n",
        "        y = np.argmax(y, axis=1)\n",
        "\n",
        "# Data preprocessing\n",
        "print(f\"\\nüîß Preprocessing data (source: {data_source})...\")\n",
        "\n",
        "if has_existing_split:\n",
        "    # Preprocess train and test separately (preserving splits)\n",
        "    print(\"   Processing train and test sets separately...\")\n",
        "    \n",
        "    # Handle NaN and infinity for train\n",
        "    if np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)):\n",
        "        print(\"   Cleaning NaN and infinity values in train set...\")\n",
        "        X_train = np.where(np.isinf(X_train), np.nan, X_train)\n",
        "        imputer_train = SimpleImputer(strategy='median')\n",
        "        X_train = imputer_train.fit_transform(X_train)\n",
        "    \n",
        "    # Handle NaN and infinity for test\n",
        "    if np.any(np.isnan(X_test)) or np.any(np.isinf(X_test)):\n",
        "        print(\"   Cleaning NaN and infinity values in test set...\")\n",
        "        X_test = np.where(np.isinf(X_test), np.nan, X_test)\n",
        "        # Use train imputer for test set\n",
        "        if 'imputer_train' in locals():\n",
        "            X_test = imputer_train.transform(X_test)\n",
        "        else:\n",
        "            imputer_test = SimpleImputer(strategy='median')\n",
        "            X_test = imputer_test.fit_transform(X_test)\n",
        "    \n",
        "    # Scale features (fit on train, transform test)\n",
        "    print(\"   Scaling features (fit on train, transform test)...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "    print(f\"‚úÖ Using existing train/test splits from NPZ file\")\n",
        "else:\n",
        "    # Handle NaN and infinity for combined data\n",
        "    if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n",
        "        print(\"   Cleaning NaN and infinity values...\")\n",
        "        X = np.where(np.isinf(X), np.nan, X)\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        X = imputer.fit_transform(X)\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    \n",
        "    # Create new train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"‚úÖ Created new train/test split\")\n",
        "\n",
        "print(f\"\\n‚úÖ Data preprocessing complete\")\n",
        "print(f\"   Train: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"   Test: X={X_test.shape}, y={y_test.shape}\")\n",
        "print(f\"   Classes: {len(np.unique(y_train))} (distribution: {np.bincount(y_train)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Model Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "print(\"ü§ñ Training Models\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'XGBoost': xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        verbosity=0\n",
        "    ),\n",
        "    'LightGBM': lgb.LGBMClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        verbose=-1\n",
        "    ),\n",
        "    'SVM': SVC(\n",
        "        kernel='rbf',\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nüîÅ Training {name}...\")\n",
        "    \n",
        "    try:\n",
        "        # Train\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Predict\n",
        "        y_pred = model.predict(X_test)\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        \n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'predictions': y_pred\n",
        "        }\n",
        "        \n",
        "        print(f\"   ‚úÖ Accuracy: {accuracy:.4f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"üìä Model Performance Summary:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Sort by accuracy\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
        "\n",
        "for name, res in sorted_results:\n",
        "    print(f\"{name:20s}: {res['accuracy']:.4f}\")\n",
        "\n",
        "if sorted_results:\n",
        "    best_name, best_result = sorted_results[0]\n",
        "    print(f\"\\nüèÜ Best Model: {best_name} (Accuracy: {best_result['accuracy']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Bootstrap Confidence Intervals (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bootstrap_confidence_interval(model, X_test, y_test, n_bootstrap=300, confidence=0.95):\n",
        "    \"\"\"\n",
        "    Calculate bootstrap confidence intervals for model accuracy.\n",
        "    \"\"\"\n",
        "    n_samples = len(y_test)\n",
        "    accuracies = []\n",
        "    \n",
        "    print(f\"üîÑ Running {n_bootstrap} bootstrap iterations...\")\n",
        "    \n",
        "    for i in range(n_bootstrap):\n",
        "        # Bootstrap sample\n",
        "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        X_boot = X_test[indices]\n",
        "        y_boot = y_test[indices]\n",
        "        \n",
        "        # Predict\n",
        "        y_pred = model.predict(X_boot)\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        acc = accuracy_score(y_boot, y_pred)\n",
        "        accuracies.append(acc)\n",
        "        \n",
        "        # Progress indicator\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"   Completed {i + 1}/{n_bootstrap} iterations\")\n",
        "    \n",
        "    # Calculate statistics\n",
        "    accuracies = np.array(accuracies)\n",
        "    mean_acc = np.mean(accuracies)\n",
        "    std_acc = np.std(accuracies)\n",
        "    \n",
        "    # Calculate confidence interval\n",
        "    alpha = 1 - confidence\n",
        "    lower = np.percentile(accuracies, 100 * alpha / 2)\n",
        "    upper = np.percentile(accuracies, 100 * (1 - alpha / 2))\n",
        "    \n",
        "    return {\n",
        "        'mean': mean_acc,\n",
        "        'std': std_acc,\n",
        "        'lower': lower,\n",
        "        'upper': upper,\n",
        "        'confidence': confidence\n",
        "    }\n",
        "\n",
        "# Run bootstrap CI for top models\n",
        "if results:\n",
        "    print(\"\\nüìä Bootstrap Confidence Intervals for Top Models\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Get top 3 models\n",
        "    top_models = sorted_results[:3]\n",
        "    \n",
        "    bootstrap_results = {}\n",
        "    \n",
        "    for name, res in top_models:\n",
        "        print(f\"\\nüîç Analyzing {name}...\")\n",
        "        ci = bootstrap_confidence_interval(res['model'], X_test, y_test, n_bootstrap=300)\n",
        "        bootstrap_results[name] = ci\n",
        "        \n",
        "        print(f\"   Mean Accuracy: {ci['mean']:.4f} ¬± {ci['std']:.4f}\")\n",
        "        print(f\"   {int(ci['confidence']*100)}% CI: [{ci['lower']:.4f}, {ci['upper']:.4f}]\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Bootstrap analysis complete\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trained models available for bootstrap analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Save Results (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Save best model\n",
        "if results:\n",
        "    best_name, best_result = sorted_results[0]\n",
        "    \n",
        "    # Save model\n",
        "    model_filename = f'best_model_{best_name.replace(\" \", \"_\")}.pkl'\n",
        "    joblib.dump(best_result['model'], model_filename)\n",
        "    print(f\"üíæ Saved best model: {model_filename}\")\n",
        "    \n",
        "    # Save results summary\n",
        "    summary = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'best_model': best_name,\n",
        "        'best_accuracy': float(best_result['accuracy']),\n",
        "        'all_results': {name: float(res['accuracy']) for name, res in results.items()}\n",
        "    }\n",
        "    \n",
        "    if 'bootstrap_results' in locals():\n",
        "        summary['bootstrap'] = {\n",
        "            name: {k: float(v) for k, v in ci.items() if k != 'confidence'}\n",
        "            for name, ci in bootstrap_results.items()\n",
        "        }\n",
        "    \n",
        "    summary_filename = 'training_results.json'\n",
        "    with open(summary_filename, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    \n",
        "    print(f\"üíæ Saved results summary: {summary_filename}\")\n",
        "    print(\"\\nüìÅ Files saved to current directory\")\n",
        "    print(\"   üí° To download: Right-click file ‚Üí Download\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results to save\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Notes\n",
        "\n",
        "- **Runtime Management**: If you encounter memory issues, restart the runtime (Runtime ‚Üí Restart runtime)\n",
        "- **GPU Usage**: To enable GPU acceleration, go to Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\n",
        "- **Data Upload**: Upload `preprocessed_data.npz` or `fallback_data.csv` to the Colab file system using the file browser\n",
        "- **Download Results**: Right-click on saved files in the file browser to download them\n",
        "- **Long Training**: For long training sessions, consider using Colab Pro for longer runtime sessions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
